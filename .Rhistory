###### Gathering Data  ######
###World cities dataset 1/3 (worldcities2013)###
# it stems from a private company, Maxmind Inc.
#http://download.maxmind.com/download/worldcities/worldcitiespop.txt.gz and transformed into CSV, since it is not available
#in .csv format right away.
worldcities2013 <- read.csv("Downloaded_Data/worldcitiespop.csv")
###World cities dataset 2/3 (worldcities1000)###
#http://download.geonames.org/export/dump/cities1000.zip and transformed into CSV, since it is not available
#in .csv format right away. The GeoNames geographical database is available for download free of charge under
#a creative commons attribution license.
worldcities1000 <- read.csv("Downloaded_Data/worldcitiespop1000.csv")
# the first both city data sets are very similar, this brought together right away as worldcities2013
worldcities1000 <- subset(worldcities1000, select=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), !is.na(V5) & V15!=0)
worldcities1000$V3 <- tolower(worldcities1000$V3)
worldcities1000$V9 <- tolower(worldcities1000$V9)
worldcities2013 <- merge(worldcities2013, worldcities1000, by.x=c("Country", "City", "AccentCity", "Region", "Latitude", "Longitude", "Population"),
by.y=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), all=TRUE)
rm(worldcities1000)
worldcities2013 <- worldcities2013[order(-worldcities2013$Population),]
worldcities2013["merge"] <- paste(worldcities2013$Country, worldcities2013$City, sep="xxx")
worldcities2013 <- worldcities2013[!duplicated(worldcities2013$merge),]
worldcities2013$merge <-NULL
###World cities dataset 3/3 (world.cities from the 'maps' package)###
#The dataset is called world.cities. Since it is from 2009, we call it worldcities2009.
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
###### Cleaning Data  ######
#Our standard for city names is lowercase letters with no special characters from the worldcities2009 dataset
#for easier merging purposes, and lowercase country names (no special charackters) from the world.cities2013 dataset.
###World cities dataset 1/2 (worldcities2013)###
#introduce Akkaraipattu as it was missing in the original dataset
worldcities2013 <- rbind(worldcities2013, data.frame(X=0,Country="lk", City="Akkaraipattu", AccentCity="Akkaraipattu",
Region= 31, Latitude=7.227862, Longitude=81.850551,Population=35000))
# Change Country Code to Countries on the level of the other set
source('SmallScripts/CountryCode.R')
worldcities2013$Country <- tolower(worldcities2013$Country)
#Recoding city names to our standard
worldcities2013$City <- gsub(" ", "", worldcities2013$City)
worldcities2013$City <- tolower(worldcities2013$City)
###World cities dataset 2/2 (worldcities2009 from the 'maps' package)###
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
worldcities2009$name <- tolower(worldcities2009$name)
#the dataset has missing or wrong information
worldcities2009$capital[worldcities2009$name == "delhi" & worldcities2009$country.etc == "India"] <- "1"
worldcities2009$name[worldcities2009$name == "soul" & worldcities2009$country.etc == "Korea South"] <- "seoul"
worldcities2009$name[worldcities2009$name == "bombay" &  worldcities2009$country.etc  == "India"] <- "mumbai"
worldcities2009$name[worldcities2009$name == "new york" &  worldcities2009$country.etc  == "USA"] <- "new york city"
#Reaching/Defining country and city standard
worldcities2009$country.etc[worldcities2009$country.etc == "Russia"] <- "Russian Federation"
worldcities2009$country.etc[worldcities2009$country.etc == "UK"] <- "United Kingdom"
worldcities2009$country.etc[worldcities2009$country.etc == "USA"] <- "United States of America"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea North"] <- "Korea, Democratic People's Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea South"] <- "Korea, Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Sicily"] <- "Italy"
worldcities2009$country.etc[worldcities2009$country.etc == "East Timor"] <- "Timor-Leste"
worldcities2009$country.etc[worldcities2009$country.etc == "Madeira"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Madiera"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Cape Verde"] <- "caboverde"
worldcities2009$country.etc<-gsub(" ", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc<-gsub("\\,", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc <- tolower(worldcities2009$country.etc)
worldcities2009$name <- gsub(" ", "", worldcities2009$name)
worldcities2009$name <- tolower(worldcities2009$name)
###### Manipulate Data  ######
#We now merge the three datasets into one. First, some preparation.
#Ordering to later enable selection on duplicates
worldcities2013 <- worldcities2013[order(-worldcities2013$Population, na.last=TRUE) , ]
worldcities2009 <- worldcities2009[order(-worldcities2009$pop, na.last=TRUE) , ]
#Creating a subset with matching columns
worldcities2013 <- subset(worldcities2013, select =c("City", "Country", "Population", "Latitude", "Longitude", "Region"))
colnames(worldcities2013)[1] <- "name"
colnames(worldcities2013)[2] <- "country.etc"
colnames(worldcities2013)[3] <- "pop"
colnames(worldcities2013)[4] <- "lat"
colnames(worldcities2013)[5] <- "long"
#Creating a column to merge over: countrycity
worldcities2009$merge <- paste(worldcities2009$country.etc, worldcities2009$name, sep="")
worldcities2013$merge <- paste(worldcities2013$country.etc, worldcities2013$name, sep="")
#Merging to new set: "world.cities"
world.cities <- merge(worldcities2009, worldcities2013, by= c("merge", "name", "country.etc", "pop", "lat", "long"), all=TRUE)
world.cities <- world.cities[order(world.cities$merge, world.cities$capital, world.cities$pop),]
world.cities <- world.cities[!duplicated(world.cities$merge), ]
world.cities <- world.cities[order(-world.cities$pop), ]
rm(worldcities2013, worldcities2009)
#Bringing country names in combined world.cities to WDI lowercase standard
source('SmallScripts/CountryCleaninginCitySet.R')
X <- world.cities$country.etc
source('SmallScripts/CleanSpecialCharacters.R')
world.cities$country.etc <- X
rm(X)
#create cache world.cities.csv
write.csv(world.cities, "Cache/world.cities.csv")
}
)
if(file.exists("Cache/world.cities.csv")){world.cities <- read.csv("Cache/world.cities.csv")}else {
############## City Data  ##############
#The purpose of this script is to gather, clean, and merge three different datasets on cities of the world with similar information.
#We merge them to increase the total number of cities that could match the GTD.
###### Gathering Data  ######
###World cities dataset 1/3 (worldcities2013)###
# it stems from a private company, Maxmind Inc.
#http://download.maxmind.com/download/worldcities/worldcitiespop.txt.gz and transformed into CSV, since it is not available
#in .csv format right away.
worldcities2013 <- read.csv("Downloaded_Data/worldcitiespop.csv")
###World cities dataset 2/3 (worldcities1000)###
#http://download.geonames.org/export/dump/cities1000.zip and transformed into CSV, since it is not available
#in .csv format right away. The GeoNames geographical database is available for download free of charge under
#a creative commons attribution license.
worldcities1000 <- read.csv("Downloaded_Data/worldcitiespop1000.csv")
# the first both city data sets are very similar, this brought together right away as worldcities2013
worldcities1000 <- subset(worldcities1000, select=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), !is.na(V5) & V15!=0)
worldcities1000$V3 <- tolower(worldcities1000$V3)
worldcities1000$V9 <- tolower(worldcities1000$V9)
worldcities2013 <- merge(worldcities2013, worldcities1000, by.x=c("Country", "City", "AccentCity", "Region", "Latitude", "Longitude", "Population"),
by.y=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), all=TRUE)
rm(worldcities1000)
worldcities2013 <- worldcities2013[order(-worldcities2013$Population),]
worldcities2013["merge"] <- paste(worldcities2013$Country, worldcities2013$City, sep="xxx")
worldcities2013 <- worldcities2013[!duplicated(worldcities2013$merge),]
worldcities2013$merge <-NULL
###World cities dataset 3/3 (world.cities from the 'maps' package)###
#The dataset is called world.cities. Since it is from 2009, we call it worldcities2009.
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
###### Cleaning Data  ######
#Our standard for city names is lowercase letters with no special characters from the worldcities2009 dataset
#for easier merging purposes, and lowercase country names (no special charackters) from the world.cities2013 dataset.
###World cities dataset 1/2 (worldcities2013)###
#introduce Akkaraipattu as it was missing in the original dataset
worldcities2013 <- rbind(worldcities2013, data.frame(X=0,Country="lk", City="Akkaraipattu", AccentCity="Akkaraipattu",
Region= 31, Latitude=7.227862, Longitude=81.850551,Population=35000))
# Change Country Code to Countries on the level of the other set
source('SmallScripts/CountryCode.R')
worldcities2013$Country <- tolower(worldcities2013$Country)
#Recoding city names to our standard
worldcities2013$City <- gsub(" ", "", worldcities2013$City)
worldcities2013$City <- tolower(worldcities2013$City)
###World cities dataset 2/2 (worldcities2009 from the 'maps' package)###
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
worldcities2009$name <- tolower(worldcities2009$name)
#the dataset has missing or wrong information
worldcities2009$capital[worldcities2009$name == "delhi" & worldcities2009$country.etc == "India"] <- "1"
worldcities2009$name[worldcities2009$name == "soul" & worldcities2009$country.etc == "Korea South"] <- "seoul"
worldcities2009$name[worldcities2009$name == "bombay" &  worldcities2009$country.etc  == "India"] <- "mumbai"
worldcities2009$name[worldcities2009$name == "new york" &  worldcities2009$country.etc  == "USA"] <- "new york city"
#Reaching/Defining country and city standard
worldcities2009$country.etc[worldcities2009$country.etc == "Russia"] <- "Russian Federation"
worldcities2009$country.etc[worldcities2009$country.etc == "UK"] <- "United Kingdom"
worldcities2009$country.etc[worldcities2009$country.etc == "USA"] <- "United States of America"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea North"] <- "Korea, Democratic People's Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea South"] <- "Korea, Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Sicily"] <- "Italy"
worldcities2009$country.etc[worldcities2009$country.etc == "East Timor"] <- "Timor-Leste"
worldcities2009$country.etc[worldcities2009$country.etc == "Madeira"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Madiera"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Cape Verde"] <- "caboverde"
worldcities2009$country.etc<-gsub(" ", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc<-gsub("\\,", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc <- tolower(worldcities2009$country.etc)
worldcities2009$name <- gsub(" ", "", worldcities2009$name)
worldcities2009$name <- tolower(worldcities2009$name)
###### Manipulate Data  ######
#We now merge the three datasets into one. First, some preparation.
#Ordering to later enable selection on duplicates
worldcities2013 <- worldcities2013[order(-worldcities2013$Population, na.last=TRUE) , ]
worldcities2009 <- worldcities2009[order(-worldcities2009$pop, na.last=TRUE) , ]
#Creating a subset with matching columns
worldcities2013 <- subset(worldcities2013, select =c("City", "Country", "Population", "Latitude", "Longitude", "Region"))
colnames(worldcities2013)[1] <- "name"
colnames(worldcities2013)[2] <- "country.etc"
colnames(worldcities2013)[3] <- "pop"
colnames(worldcities2013)[4] <- "lat"
colnames(worldcities2013)[5] <- "long"
#Creating a column to merge over: countrycity
worldcities2009$merge <- paste(worldcities2009$country.etc, worldcities2009$name, sep="")
worldcities2013$merge <- paste(worldcities2013$country.etc, worldcities2013$name, sep="")
#Merging to new set: "world.cities"
world.cities <- merge(worldcities2009, worldcities2013, by= c("merge", "name", "country.etc", "pop", "lat", "long"), all=TRUE)
world.cities <- world.cities[order(world.cities$merge, world.cities$capital, world.cities$pop),]
world.cities <- world.cities[!duplicated(world.cities$merge), ]
world.cities <- world.cities[order(-world.cities$pop), ]
rm(worldcities2013, worldcities2009)
#Bringing country names in combined world.cities to WDI lowercase standard
source('SmallScripts/CountryCleaninginCitySet.R')
X <- world.cities$country.etc
source('SmallScripts/CleanSpecialCharacters.R')
world.cities$country.etc <- X
rm(X)
#create cache world.cities.csv
write.csv(world.cities, "Cache/world.cities.csv")
}}
setwd("C:/Users/Lokus/Dropbox/UrbanTerror")
if(file.exists("Cache/world.cities.csv")){world.cities <- read.csv("Cache/world.cities.csv")}else {
############## City Data  ##############
#The purpose of this script is to gather, clean, and merge three different datasets on cities of the world with similar information.
#We merge them to increase the total number of cities that could match the GTD.
###### Gathering Data  ######
###World cities dataset 1/3 (worldcities2013)###
# it stems from a private company, Maxmind Inc.
#http://download.maxmind.com/download/worldcities/worldcitiespop.txt.gz and transformed into CSV, since it is not available
#in .csv format right away.
worldcities2013 <- read.csv("Downloaded_Data/worldcitiespop.csv")
###World cities dataset 2/3 (worldcities1000)###
#http://download.geonames.org/export/dump/cities1000.zip and transformed into CSV, since it is not available
#in .csv format right away. The GeoNames geographical database is available for download free of charge under
#a creative commons attribution license.
worldcities1000 <- read.csv("Downloaded_Data/worldcitiespop1000.csv")
# the first both city data sets are very similar, this brought together right away as worldcities2013
worldcities1000 <- subset(worldcities1000, select=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), !is.na(V5) & V15!=0)
worldcities1000$V3 <- tolower(worldcities1000$V3)
worldcities1000$V9 <- tolower(worldcities1000$V9)
worldcities2013 <- merge(worldcities2013, worldcities1000, by.x=c("Country", "City", "AccentCity", "Region", "Latitude", "Longitude", "Population"),
by.y=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), all=TRUE)
rm(worldcities1000)
worldcities2013 <- worldcities2013[order(-worldcities2013$Population),]
worldcities2013["merge"] <- paste(worldcities2013$Country, worldcities2013$City, sep="xxx")
worldcities2013 <- worldcities2013[!duplicated(worldcities2013$merge),]
worldcities2013$merge <-NULL
###World cities dataset 3/3 (world.cities from the 'maps' package)###
#The dataset is called world.cities. Since it is from 2009, we call it worldcities2009.
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
###### Cleaning Data  ######
#Our standard for city names is lowercase letters with no special characters from the worldcities2009 dataset
#for easier merging purposes, and lowercase country names (no special charackters) from the world.cities2013 dataset.
###World cities dataset 1/2 (worldcities2013)###
#introduce Akkaraipattu as it was missing in the original dataset
worldcities2013 <- rbind(worldcities2013, data.frame(X=0,Country="lk", City="Akkaraipattu", AccentCity="Akkaraipattu",
Region= 31, Latitude=7.227862, Longitude=81.850551,Population=35000))
# Change Country Code to Countries on the level of the other set
source('SmallScripts/CountryCode.R')
worldcities2013$Country <- tolower(worldcities2013$Country)
#Recoding city names to our standard
worldcities2013$City <- gsub(" ", "", worldcities2013$City)
worldcities2013$City <- tolower(worldcities2013$City)
###World cities dataset 2/2 (worldcities2009 from the 'maps' package)###
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
worldcities2009$name <- tolower(worldcities2009$name)
#the dataset has missing or wrong information
worldcities2009$capital[worldcities2009$name == "delhi" & worldcities2009$country.etc == "India"] <- "1"
worldcities2009$name[worldcities2009$name == "soul" & worldcities2009$country.etc == "Korea South"] <- "seoul"
worldcities2009$name[worldcities2009$name == "bombay" &  worldcities2009$country.etc  == "India"] <- "mumbai"
worldcities2009$name[worldcities2009$name == "new york" &  worldcities2009$country.etc  == "USA"] <- "new york city"
#Reaching/Defining country and city standard
worldcities2009$country.etc[worldcities2009$country.etc == "Russia"] <- "Russian Federation"
worldcities2009$country.etc[worldcities2009$country.etc == "UK"] <- "United Kingdom"
worldcities2009$country.etc[worldcities2009$country.etc == "USA"] <- "United States of America"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea North"] <- "Korea, Democratic People's Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea South"] <- "Korea, Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Sicily"] <- "Italy"
worldcities2009$country.etc[worldcities2009$country.etc == "East Timor"] <- "Timor-Leste"
worldcities2009$country.etc[worldcities2009$country.etc == "Madeira"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Madiera"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Cape Verde"] <- "caboverde"
worldcities2009$country.etc<-gsub(" ", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc<-gsub("\\,", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc <- tolower(worldcities2009$country.etc)
worldcities2009$name <- gsub(" ", "", worldcities2009$name)
worldcities2009$name <- tolower(worldcities2009$name)
###### Manipulate Data  ######
#We now merge the three datasets into one. First, some preparation.
#Ordering to later enable selection on duplicates
worldcities2013 <- worldcities2013[order(-worldcities2013$Population, na.last=TRUE) , ]
worldcities2009 <- worldcities2009[order(-worldcities2009$pop, na.last=TRUE) , ]
#Creating a subset with matching columns
worldcities2013 <- subset(worldcities2013, select =c("City", "Country", "Population", "Latitude", "Longitude", "Region"))
colnames(worldcities2013)[1] <- "name"
colnames(worldcities2013)[2] <- "country.etc"
colnames(worldcities2013)[3] <- "pop"
colnames(worldcities2013)[4] <- "lat"
colnames(worldcities2013)[5] <- "long"
#Creating a column to merge over: countrycity
worldcities2009$merge <- paste(worldcities2009$country.etc, worldcities2009$name, sep="")
worldcities2013$merge <- paste(worldcities2013$country.etc, worldcities2013$name, sep="")
#Merging to new set: "world.cities"
world.cities <- merge(worldcities2009, worldcities2013, by= c("merge", "name", "country.etc", "pop", "lat", "long"), all=TRUE)
world.cities <- world.cities[order(world.cities$merge, world.cities$capital, world.cities$pop),]
world.cities <- world.cities[!duplicated(world.cities$merge), ]
world.cities <- world.cities[order(-world.cities$pop), ]
rm(worldcities2013, worldcities2009)
#Bringing country names in combined world.cities to WDI lowercase standard
source('SmallScripts/CountryCleaninginCitySet.R')
X <- world.cities$country.etc
source('SmallScripts/CleanSpecialCharacters.R')
world.cities$country.etc <- X
rm(X)
#create cache world.cities.csv
write.csv(world.cities, "Cache/world.cities.csv")
}
#Renaming columns and select subsets for merging over fake variable to create a matrix
#with each City X each Urban Center (~ 60.000 Cities X ~ 500 urban Centers)
UCmerge <- subset(UrbanCenters, select = c("lon", "lat", "full.name"))
UCmerge$fake=1
WCmerge <-subset(world.cities, select = c("long", "lat"))
WCmerge["CityID"] <- rownames(world.cities)
WCmerge$fake=1
#############################################################################################################
######################################### 1. Urban Centers ##################################################
#############################################################################################################
if(file.exists("Cache/UrbanCenters.csv"))
{UrbanCenters <- read.csv("Cache/UrbanCenters.csv")
} else
{
##### Urban Centers #####
#We use a Wikipedia article on global Urban Centers for our analysis on the geographical distance from the location of attacks
#to the nearest, "important" city. We then use the google.maps API to geolocate each of the centers.
###### Gathering Data  ######
# Scrap Wiki on Urban Centers using the 'XML' package
URL <- 'http://en.wikipedia.org/w/index.php?title=List_of_urban_areas_by_population&section=2'
table <- readHTMLTable(URL, encoding = "UTF-16")
UrbanCenters <- table [[2]]
UrbanCenters$City <- as.character(UrbanCenters$City)
colnames(UrbanCenters)[5] <- "Area"
colnames(UrbanCenters)[6] <- "Density"
##### Cleaning Data #####
#Cleaning the Urban Centers name in order to allow google.maps API to find them
source('SmallScripts/UrbanCleaning.R')
###### Manipulate Data ######
#Putting in a string with "Country, City" to allow google.maps API to find them
CoCi <-data.frame(paste(UrbanCenters$Country, UrbanCenters$City, sep=", "), row.names = NULL)
CoCi["loc"] <- CoCi
CoCi$loc <- as.character(CoCi$loc)
CoCi$loc <- gsub("^..", "", CoCi$loc)
CoCiLoc<-(CoCi$loc)
#Looking up lon/lat data for each Urban Center via google.maps using the geocode function of the package maps.
#This is a very time consuming process, which is why we chose to cache the result to reduce computation time for this script.
if(file.exists("Cache/UrbanLoc.csv"))
{UrbanLoc <- read.csv("Cache/UrbanLoc.csv")
} else
{
UrbanLoc <- geocode(CoCiLoc, output = c("latlon", "latlona", "more", "all"),messaging = FALSE, sensor = FALSE, override_limit = FALSE)
write.csv(UrbanLoc, "Cache/UrbanLoc.csv")
}
#Inlcuding geographic data into the original Urban Centers data frame
UrbanCenters["lat"] <- UrbanLoc$lat
UrbanCenters["lon"] <- UrbanLoc$lon
UrbanCenters["full.name"] <- CoCiLoc
UrbanCenters$City <- tolower(UrbanCenters$City)
# remove commas
UrbanCenters$Population <- gsub("\\,","",UrbanCenters$Population)
UrbanCenters$Area <- gsub("\\,","",UrbanCenters$Area)
UrbanCenters$Density <- gsub("\\,","",UrbanCenters$Density)
# Assigning a 1 to all the largest urban centers in a country
UrbanCenters$Population <- as.numeric(UrbanCenters$Population)
LUC <- aggregate(Population ~ Country,UrbanCenters,max)
LUC["largest.UC"] <- 1
UrbanCenters <- merge(UrbanCenters, LUC, by=c("Population", "Country"), all.x=TRUE)
UrbanCenters <- UrbanCenters[order(-UrbanCenters$Population), ]
UrbanCenters$largest.UC[is.na(UrbanCenters$largest.UC)] <-0
rm(LUC)
#Including dummy variables for coastal megacities
source('SmallScripts/CoastalMC.R')
#Deleting what is not needed anymore
rm(UrbanLoc, table ,URL, CoCi, CoCiLoc)
#Caching the Urban Centers dataframe
write.csv(UrbanCenters, "Cache/UrbanCenters.csv")
}
#############################################################################################################
###################################### 2. Three City Data Sets ###############################################
#############################################################################################################
if(file.exists("Cache/world.cities.csv"))
{world.cities <- read.csv("Cache/world.cities.csv")
} else
{
############## City Data  ##############
#The purpose of this script is to gather, clean, and merge three different datasets on cities of the world with similar information.
#We merge them to increase the total number of cities that could match the GTD.
###### Gathering Data  ######
###World cities dataset 1/3 (worldcities2013)###
# it stems from a private company, Maxmind Inc.
#http://download.maxmind.com/download/worldcities/worldcitiespop.txt.gz and transformed into CSV, since it is not available
#in .csv format right away.
worldcities2013 <- read.csv("Downloaded_Data/worldcitiespop.csv")
###World cities dataset 2/3 (worldcities1000)###
#http://download.geonames.org/export/dump/cities1000.zip and transformed into CSV, since it is not available
#in .csv format right away. The GeoNames geographical database is available for download free of charge under
#a creative commons attribution license.
worldcities1000 <- read.csv("Downloaded_Data/worldcitiespop1000.csv")
# the first both city data sets are very similar, this brought together right away as worldcities2013
worldcities1000 <- subset(worldcities1000, select=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), !is.na(V5) & V15!=0)
worldcities1000$V3 <- tolower(worldcities1000$V3)
worldcities1000$V9 <- tolower(worldcities1000$V9)
worldcities2013 <- merge(worldcities2013, worldcities1000, by.x=c("Country", "City", "AccentCity", "Region", "Latitude", "Longitude", "Population"),
by.y=c("V9", "V3", "V2", "V18", "V5", "V6", "V15"), all=TRUE)
rm(worldcities1000)
worldcities2013 <- worldcities2013[order(-worldcities2013$Population),]
worldcities2013["merge"] <- paste(worldcities2013$Country, worldcities2013$City, sep="xxx")
worldcities2013 <- worldcities2013[!duplicated(worldcities2013$merge),]
worldcities2013$merge <-NULL
###World cities dataset 3/3 (world.cities from the 'maps' package)###
#The dataset is called world.cities. Since it is from 2009, we call it worldcities2009.
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
###### Cleaning Data  ######
#Our standard for city names is lowercase letters with no special characters from the worldcities2009 dataset
#for easier merging purposes, and lowercase country names (no special charackters) from the world.cities2013 dataset.
###World cities dataset 1/2 (worldcities2013)###
#introduce Akkaraipattu as it was missing in the original dataset
worldcities2013 <- rbind(worldcities2013, data.frame(X=0,Country="lk", City="Akkaraipattu", AccentCity="Akkaraipattu",
Region= 31, Latitude=7.227862, Longitude=81.850551,Population=35000))
# Change Country Code to Countries on the level of the other set
source('SmallScripts/CountryCode.R')
worldcities2013$Country <- tolower(worldcities2013$Country)
#Recoding city names to our standard
worldcities2013$City <- gsub(" ", "", worldcities2013$City)
worldcities2013$City <- tolower(worldcities2013$City)
###World cities dataset 2/2 (worldcities2009 from the 'maps' package)###
data(world.cities)
worldcities2009 <- world.cities
rm(world.cities)
worldcities2009$name <- tolower(worldcities2009$name)
#the dataset has missing or wrong information
worldcities2009$capital[worldcities2009$name == "delhi" & worldcities2009$country.etc == "India"] <- "1"
worldcities2009$name[worldcities2009$name == "soul" & worldcities2009$country.etc == "Korea South"] <- "seoul"
worldcities2009$name[worldcities2009$name == "bombay" &  worldcities2009$country.etc  == "India"] <- "mumbai"
worldcities2009$name[worldcities2009$name == "new york" &  worldcities2009$country.etc  == "USA"] <- "new york city"
#Reaching/Defining country and city standard
worldcities2009$country.etc[worldcities2009$country.etc == "Russia"] <- "Russian Federation"
worldcities2009$country.etc[worldcities2009$country.etc == "UK"] <- "United Kingdom"
worldcities2009$country.etc[worldcities2009$country.etc == "USA"] <- "United States of America"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea North"] <- "Korea, Democratic People's Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Korea South"] <- "Korea, Republic of"
worldcities2009$country.etc[worldcities2009$country.etc == "Sicily"] <- "Italy"
worldcities2009$country.etc[worldcities2009$country.etc == "East Timor"] <- "Timor-Leste"
worldcities2009$country.etc[worldcities2009$country.etc == "Madeira"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Madiera"] <- "Portugal"
worldcities2009$country.etc[worldcities2009$country.etc == "Cape Verde"] <- "caboverde"
worldcities2009$country.etc<-gsub(" ", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc<-gsub("\\,", "",worldcities2009$country.etc, ignore.case=TRUE)
worldcities2009$country.etc <- tolower(worldcities2009$country.etc)
worldcities2009$name <- gsub(" ", "", worldcities2009$name)
worldcities2009$name <- tolower(worldcities2009$name)
###### Manipulate Data  ######
#We now merge the three datasets into one. First, some preparation.
#Ordering to later enable selection on duplicates
worldcities2013 <- worldcities2013[order(-worldcities2013$Population, na.last=TRUE) , ]
worldcities2009 <- worldcities2009[order(-worldcities2009$pop, na.last=TRUE) , ]
#Creating a subset with matching columns
worldcities2013 <- subset(worldcities2013, select =c("City", "Country", "Population", "Latitude", "Longitude", "Region"))
colnames(worldcities2013)[1] <- "name"
colnames(worldcities2013)[2] <- "country.etc"
colnames(worldcities2013)[3] <- "pop"
colnames(worldcities2013)[4] <- "lat"
colnames(worldcities2013)[5] <- "long"
#Creating a column to merge over: countrycity
worldcities2009$merge <- paste(worldcities2009$country.etc, worldcities2009$name, sep="")
worldcities2013$merge <- paste(worldcities2013$country.etc, worldcities2013$name, sep="")
#Merging to new set: "world.cities"
world.cities <- merge(worldcities2009, worldcities2013, by= c("merge", "name", "country.etc", "pop", "lat", "long"), all=TRUE)
world.cities <- world.cities[order(world.cities$merge, world.cities$capital, world.cities$pop),]
world.cities <- world.cities[!duplicated(world.cities$merge), ]
world.cities <- world.cities[order(-world.cities$pop), ]
rm(worldcities2013, worldcities2009)
#Bringing country names in combined world.cities to WDI lowercase standard
source('SmallScripts/CountryCleaninginCitySet.R')
X <- world.cities$country.etc
source('SmallScripts/CleanSpecialCharacters.R')
world.cities$country.etc <- X
rm(X)
#create cache world.cities.csv
write.csv(world.cities, "Cache/world.cities.csv")
}
#############################################################################################################
####################### 3. Merging Data Both City Data Sets and Urban Centers ###############################
#############################################################################################################
#In this script we merge the UrbanCenters and world.cities databases so that each
#city is allocated to its closest Urban Center (including distances)
#Renaming columns and select subsets for merging over fake variable to create a matrix
#with each City X each Urban Center (~ 60.000 Cities X ~ 500 urban Centers)
UCmerge <- subset(UrbanCenters, select = c("lon", "lat", "full.name"))
UCmerge$fake=1
WCmerge <-subset(world.cities, select = c("long", "lat"))
WCmerge["CityID"] <- rownames(world.cities)
WCmerge$fake=1
ALLDIST <-merge(UCmerge, WCmerge, by=c("fake"))
View(world.cities)
set.seed(600000)
split(world.cities, 600000)
View(world.cities)
world.cities1 <- subset (world.cities, X<=600000)
View(world.cities)
View(world.cities)
world.cities1 <- subset (world.cities, pop<=mean(pop))
world.cities1 <- subset (world.cities, pop<=median(pop))
world.cities2 <- subset (world.cities, pop>=median(pop))
world.cities2 <- subset (world.cities, !pop>=median(pop))
View(world.cities1)
#In this script we merge the UrbanCenters and world.cities databases so that each
#city is allocated to its closest Urban Center (including distances)
#Renaming columns and select subsets for merging over fake variable to create a matrix
#with each City X each Urban Center (~ 60.000 Cities X ~ 500 urban Centers)
UCmerge <- subset(UrbanCenters, select = c("lon", "lat", "full.name"))
UCmerge$fake=1
WCmerge <-subset(world.cities1, select = c("long", "lat"))
WCmerge["CityID"] <- rownames(world.cities1)
WCmerge$fake=1
ALLDIST <-merge(UCmerge, WCmerge, by=c("fake"))
#Finding each distance ( ~ 30 million individual distances will be found )
ALLDIST["DISTkm"] <- gdist(ALLDIST$lon, ALLDIST$lat.x, ALLDIST$long, ALLDIST$lat.y, units = "km",
a = 6378137, b = 6356752, verbose = FALSE)
#Reducing to only the closest Urban Center for each and every city, 30 million distances to the ~ 60.000 minimal ones
ALLDIST.min <- aggregate(DISTkm ~ CityID, ALLDIST, function(x) min(x))
